{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>How to Predict the Prices of Used Cars</h1>\n<p>Subtitle: Using supervised learning to build a price-prediction algorithm</p>\n<p style=\"margin-bottom:0 !important;\">Created: 2020-05-2020</p>\n<p>Author: <a href=\"https://sthakur.work\">Shambhavi Thakur</a></p>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nWhen we want to sell used cars, one of the biggest problems is deciding reasonable selling prices for the cars. An effective way to solve this problem is to use a machine-learning model that can predict car prices.\n\nIn this notebook, I use Python code and libraries to build a price-prediction model. The notebook is an implementation of the project [Predicting the Prices of Used Cars](https://sthakur.work/project/predicting-prices-of-used-cars/).\n\nLet us understand the objectives and implemention strategy of the project first. Subsequently, we will discuss and run the project&nbsp;code.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Project objectives\n\nThe main objectives of this project are as follows:\n\n- Identify relevant machine-learning algorithms for the project.\n\n- Build price-prediction models based on the chosen algorithms.\n\n- Validate the models.\n\n- Identify the most accurate model.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Implementation strategy\n\nTo achieve the project objectives, I intend to work through the following steps:\n\n1. [Identify machine-learning algorithms that can help predict car prices.](#step1-desc)<a name=\"step1\"></a>\n\n2. [Obtain a dataset about used&nbsp;cars.](#step2-desc)<a name=\"step2\"></a>\n\n3. [Explore and clean the dataset.](#step3-desc)<a name=\"step3\"></a>\n\n4. [Visualize the dataset and clean it further.](#step4-desc)<a name=\"step4\"></a>\n\n5. [Split the dataset for training and testing.](#step5-desc)<a name=\"step5\"></a>\n\n6. [Build and validate price-prediction models.](#step6-desc)<a name=\"step6\"></a>\n\n7. [Identify the most appropriate model.](#step7-desc)<a name=\"step7\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2>Step 1: Identifying machine-learning algorithms<a aria-pressed=\"true\" class=\"btn btn-primary btn-sm\" data-toggle=\"popover\" id='step1-desc' href=\"#step1\" role=\"button\" style=\"color: white; margin-left: 20px; target='_self'\">Go to steplist</a></h2>\n\nCars have been bought and sold for a long time, and there are numerous data sets that list features of cars and their prices. As we can easily obtain a data set of this type, I am opting for supervised learning in this project.\n\n### Tasks that comprise supervised learning\n\nSupervised learning is all about training an algorithm using prepared, labeled&nbsp;data. Supervised learning for car-price prediction involves the following tasks:\n\n1. Obtain a dataset that contains the attributes of various cars, including their prices and mileage.\n\n2. Split the dataset into training and testing datasets.\n\n3. Train a machine-learning model using the training dataset.\n\n4. Once the model has been trained, validate it by passing on to it all columns from the testing data except the price column. The accuracy of the algorithm depends on how close the prices it predicts are to the actual prices in the testing dataset.\n\n### Types of supervised learning\n\nSupervised learning is of two types, regression and classification. When the target variable is numerical, we have a regression problem. When the target variable is categorical, we have a classification problem.\n\nThe target variable for this project, car price, is definitely numerical. So, regression will be the most appropriate for this project.\n\n### Regression algorithms\n\nRegression algorithms are of various types. In this project, we will build and compare models based on four regression algorithms, or regressors. Here is a description of these regressors:\n\n- **Linear**: A linear regressor uses linear or step-by-step techniques to find out how a target, or dependent, variable is related to one or more independent variables.\n\n- **Random forest**: Given a set of independent variables, a random-forest regressor uses multiple decision trees—in other words, a forest of decision trees—to predict the value of the corresponding target variable. It uses the collective knowledge of the forest to determine which independent variables have the maximum impact on the target variable and predicts values accordingly.\n\n- **XGBoost**: Short for extreme gradient boosting, XGBoost is a library that lets us build highly efficient decision trees. Using an XGBoost regressor, we can train different models in tandem. Each model learns from and improves upon the model that preceded it. Collectively, all the models based on an XGBoost regressor determine the impact that independent variables have on the corresponding target variable and rank the independent variables accordingly. This ranking eventually enables the XGBoost regressor to predict target values.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2>Step 2: Obtaining a dataset<a aria-pressed=\"true\" class=\"btn btn-primary btn-sm\" data-toggle=\"popover\" id='step2-desc' href=\"#step2\" role=\"button\" style=\"color: white; margin-left: 20px; target='_self'\">Go to steplist</a></h2>\n\nWe will use a [Kaggle dataset](https://www.kaggle.com/austinreese/craigslist-carstrucks-data) for this project. This dataset contains data that was scraped recently from Craiglist in the United States&nbsp;(US).\n\nI have already added the dataset to the project environment. Here is code that lists the path to the dataset:","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('../input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Step 3: Exploring and cleaning the dataset<a aria-pressed=\"true\" class=\"btn btn-primary btn-sm\" data-toggle=\"popover\" id='step3-desc' href=\"#step3\" role=\"button\" style=\"color: white; margin-left: 20px; target='_self'\">Go to steplist</a></h2>\n\nTo explore the dataset, we will first save it as a pandas dataframe. Then, we will view the contents and properties of the dataset and, if required, clean it.\n\n### Converting the dataset into a pandas dataframe\n\nWe will begin by importing pandas, the standard data-science package.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will import the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_original = pd.read_csv('../input/craigslist-carstrucks-data/vehicles.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looking at the contents of the dataset\n\nLet us browse through the first few rows of the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\ndf = df_original.copy()\n\ndf.iloc[np.r_[0:3, -3:0]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cleaning the dataset\n\nSome of the dataset columns, such as url and region_url, seem irrelevant to this project, and some columns have NaN, or null, values. In addition, the price column, which represents our target variable (the values that our algorithm should predict), seems to lose its relevance sandwiched between all the other columns. We need to move it to the end of the dataset.\n\nLet us remove the irrelevant columns first.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"irrelevant_cols = ['id', 'url', 'region_url', 'vin', 'image_url', \\\n                   'description', 'county']\n\ndf = df.drop(columns=irrelevant_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, let us reposition the price column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"col_list = ['price']\nrearranged_cols = np.hstack((df.columns.difference(col_list, sort=False), col_list))\n\ndf = df.reindex(columns=rearranged_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is the structure of the edited dataset:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, some of the columns have been removed and the price column is at the end of the dataset. However, there are still quite a few columns with null values.\n\nLet us determine the total number of null values in each column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us also depict the distribution of the null values via a graph.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nheat_map = sns.heatmap(df.isnull(), yticklabels=False, cbar=True, cmap='Blues')\n_ = heat_map.set_xticklabels(heat_map.get_xticklabels(), color='#6eafd7')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Missing values can lead to errors in machine-learning models. To avoid these errors, we can use one of the following workarounds:\n\n- Remove each row that contains any missing values.\n\n- Replace missing values with estimates by using scikit-learn imputers.\n\nAs we want our algorithms to be accurate, we must retain as much of the car data as possible. Therefore, we will opt for the second workaround.\n\nBefore we use imputers on the dataset, however, let us organize some of its column names into two groups, numerical and categorical. We will use the corresponding columns as independent variables for our models.\n\nHere are the groups:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical = ['year', 'odometer', 'lat', 'long']\n\ncategorical = ['region', 'condition','cylinders',\\\n               'fuel', 'title_status', 'transmission',\\\n               'drive', 'size', 'type', 'paint_color', 'state']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We excluded manufacturer and model from the categorical list because we want to avoid any incorrect combinations of manufacturers and models.\n\nNext, we will make sure that all string values in the dataset are in lowercase and do not contain any unnecessary spaces.\n\nHere is the code that we will use:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in df.columns[1:]:\n    if df[column].dtype == 'object':\n        df[column] = df[column].str.lower().str.strip()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are now ready to replace all the null values in the dataset with estimated values. We will fill out the numerical columns of the dataset by using Extratreesregressor. And, to fill out the categorical columns, we will use BayesianRidge.\n\nHow do Extratreesregressor and BayesianRidge work?\n\nExtratreesregressor divides a target dataset into smaller subsets. Then, it uses multiple decision trees, or extra trees, on the subsets to determine how various attributes of the dataset interrelate. It combines the findings of the trees to generate an average value for each null&nbsp;field.\n\nAn imputer of the Extratreesregressor type prevents overfitting. In other words, it allows us to create models that can transfer what they learn from training datasets to other, real-world datasets and make highly accurate predictions.\n\nUnlike Extratreesregressor, BayesianRidge uses linear regression to determine relationships between variables. Based on these relationships, it generates regularized values for non-null fields.\n\nTo be able to use the Extratreesregressor and BayesianRidge imputers, we will first import relevant modules. We will also save the initializers of these imputers in a&nbsp;list:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.experimental import enable_iterative_imputer\n\nfrom sklearn import preprocessing\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nimputers = [\n    BayesianRidge(),\n    ExtraTreesRegressor(n_estimators=10, random_state=0),\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let us use the items in the `imputers` list to generate values.\n\nFirst, we will use Extratreesregressor to fill out the null fields of the numerical columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sr_numerical = df[numerical]\nimp_numerical = IterativeImputer(imputers[1])\nimputed_vals = imp_numerical.fit_transform(sr_numerical)\ndf[numerical] = imputed_vals","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The numerical columns do not have any null values now.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()[numerical]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will use BayesianRidge to generate values for the null fields of the categorical columns.\n\nThe BayesianRidge algorithm will generate the required values based on the data that already exist in the categorical columns. But the algorithm cannot understand the existing data in their raw form because they are strings. We will have to encode these strings first.\n\nHere is a function that can do the encoding&nbsp;for&nbsp;us:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode(data_col):\n    '''A function that transforms non-null values'''\n    vals = np.array(data_col.dropna())\n    # Reshaping the non-null data of a column\n    reshaped_data = vals.reshape(-1,1)\n    # Encoding the reshaped data\n    encoded_data = encoder.fit_transform(reshaped_data)\n    # Assigning the encoded values to the corresponding column values\n    data_col.loc[data_col.notnull()] = np.squeeze(encoded_data)\n    return data_col","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let us use the `encode` function along with other code to update the categorical columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sr_categorical = df[categorical]\nencoder = preprocessing.LabelEncoder()\n\n# Using a for loop to iterate through each categorical column and\n# filling out its null fields\nfor column in categorical:\n    encode(sr_categorical[column])\n    imp_categorical = IterativeImputer(BayesianRidge())\n    imputed_vals_cat = imp_categorical.fit_transform(sr_categorical[column].values.reshape(-1, 1))\n    imputed_vals_cat = imputed_vals_cat.astype('int64')\n    imputed_vals_cat = pd.DataFrame(imputed_vals_cat)\n    imputed_vals_cat = encoder.inverse_transform(imputed_vals_cat.values.reshape(-1, 1))\n    sr_categorical[column] = imputed_vals_cat\n\ndf[categorical]= sr_categorical","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The categorical columns also don't have any null values&nbsp;now.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()[categorical]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here are the first five rows of the updated dataset:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the 5<sup>th</sup> row, the model name seems unrealistic. There are other such model names in the dataset. For example, one model name contains a monetary value,&nbsp;$500.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[:, ['region', 'manufacturer', 'model']]\\\n[df.model.str.startswith(r'$500', na=False)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will delete such unrealistic model names later. Right now, let us delete all rows of the dataset that still have null values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How many unique values does each dataset column have now?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.apply(pd.Series.nunique)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset has a large number of car models at present. We will remove many of these models later. This step will reduce the number of unrealistic model names in our dataset. It will also ensure that the dataset has an adequate number of cars of each model—a crucial factor when training machine-learning models.\n\nAside from the number of models, another conspicuous fact is that there is one additional value in the state column. It is 'dc' and represents District of Columbia, which is a federal district rather than a state. We will let this inconsistency be. Car prices are probably different in District of Columbia compared to the rest of Washington state. Our algorithms should be able to figure this&nbsp;out.\n\nSo, the dataset is fairly clean now. Let us save it as a CSV file.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv('vehicles_eda.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will proceed to visualizing the contents of the updated dataset and cleaning it further.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2>Step 4: Visualization and further cleaning<a aria-pressed=\"true\" class=\"btn btn-primary btn-sm\" data-toggle=\"popover\" id='step4-desc' href=\"#step4\" role=\"button\" style=\"color: white; margin-left: 20px; target='_self'\">Go to steplist</a></h2>\n\nWe will begin this step by looking at the distribution of the target variable, price. We will use a seaborn distplot for this purpose.\n\nAs we want to create multiple seaborn distplots, let us write a reusable plot-drawing function first.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(color_codes=True)\nsns.set(rc={'figure.figsize':(6,3)})\n\ndef plot_histogram(col, color_val='#005c9d',\\\n                   x_label='Price [x10\\u2076 USD]', y_label='Frequency',\\\n                   title_text='Distribution of car prices'):\n    sns.distplot(col, kde=False, color=color_val)\n    \n    ax = plt.gca()\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(y_label)\n    ax.set_title(title_text)\n    ax.get_xaxis().get_major_formatter().set_scientific(False)\n    ax.get_yaxis().get_major_formatter().set_scientific(False)\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will also import the CSV file that we had saved in the previous step.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('vehicles_eda.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the `plot_histogram` function on the price column of the dataset, we get the following plot.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"price_mill = df.price/10**6\nplot_histogram(price_mill)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot indicates that the prices are skewed toward the left, with the maximum number of cars hovering around&nbsp;zero.\n\nFor a better perspective, let us view the distribution in the USD&nbsp;0–60,000 range.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_histogram(df.price[df.price<60000])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apparently, a large number of cars in the database are priced at 0. Here is code that proves this:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"price_counts = df.price.value_counts()\nprint('Price: ', price_counts.index[0], '\\nCounts: ', price_counts.values[0])\nprint('\\nTen most frequently occurring prices:\\n')\nprint(price_counts[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How do the mean and the median of the prices compare? ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean:', df.price.mean())\nprint()\nprint('Median: ', df.price.median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The minimum price is 0. What is the maximum price?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Max. price: ', df.price.max())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The huge difference between the mean and the median corroborates that there are outliers in the price data. In addition, the maximum price is unrealistic.\n\nThe minimum price is also unrealistic for us. It does not conform to our requirements. We definitely don't want our algorithms to learn about cars whose prices are pegged at 0. The model will be a price-predictor, after&nbsp;all. So, let us remove rows where the price is 0 and reset the database index.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let us look at some of the attributes of cars with the highest prices.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['region', 'year', 'manufacturer', 'model', 'price']\n\ndf.loc[:, cols][df.price>100000].sort_values(by='price', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, a 1997 Dodge RAM 3500 shouldn't cost USD 2+ billions. As per the [Cargurus](https://www.cargurus.com/Cars/l-Used-1997-Dodge-RAM-3500-c5452) website, Dodge RAM 3500s from 1998 and earlier cost less than USD 20,000. That's much lower than USD 2 billions. The extremely high price listed in our dataset is erroneous, and there are quite a few outliers like this.\n\nWe will have to remove outliers from the price column as well as other numerical columns of the dataset. We can use the interquartile range (IQR) of each column to do so. IQR is the difference between the third quartile and the first quartile of a data range—for example, a database column.\n\n#### What are first and third quartiles?\n\nSuppose we have sorted a data range and found its median value. The lower half of this range includes all the values that are less than the median, and the upper half includes values that are greater than the median.\n\nThe first quartile is the median of the values in the lower half. Almost 25% of all the values in the data range will be less than the first quartile.\n\nThe third quartile is the median of the upper half. Almost 75% of all the values in the data range will be less than the third quartile.\n\n#### How to remove outliers using IQR?\n\nTo remove outliers from a data range using IQR, we will have to remove values that are\n\n- less than the difference between the first quartile and 1.5 times the IQR\n\n- more than the difference between the third quartile and 1.5 times the IQR\n\nIf Q1 is the first quartile, Q3 is the third quartile, and IQR is their difference, outliers in the lower half of the data range will be less than the following value:\n\n`Q1 - 1.5 * IQR`\n\nAnd outliers in the upper half of the data range will be greater than the following value:\n\n\n`Q3 + 1.5 * IQR`\n\nBut using IQR on the price column as is won't give us satisfactory results. This is because the car prices in our dataset are not uniform. Most of the prices are clustered near the lowest end of the price scale and the price range is huge. This type of non-uniformity can prevent machine-learning algorithms from interpreting data effectively.\n\nSo, we will have to bring in uniformity into the dataset via an additional column. This column will contain uniformized versions of the values in the price column. To uniformize the values, we will apply a logarithmic function, log1p, to them. The logarithmic function will create a new scale for the price values. The price values will be distributed more evenly across this scale.\n\nTo create the proposed column, let us run the following code:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.insert(17, 'logprice', np.log1p(df['price']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To check whether the values in the logprice column are uniform, we will view their distribution graphically.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_histogram(df.logprice) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yes, this histogram is evenly distributed. Now, it will be easier for us to remove outliers from our dataset by using the IQR method.\n\nBut before we apply this method, we will change the datatype of the price column to `string`. This will ensure that the IQR method is not applied to the price column. Later, once we have removed outliers from the dataset, we will change the data type of its price column back to `int64`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['price'] = df.price.astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, it's time to apply the IQR method.\n\nTo begin with, we will calculate the Q1 and Q3 values and the IQRs of the numerical columns in the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Q3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IQR","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will update the dataset, applying the IQR formula to remove outliers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we will revert the data type of the price column to its original value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['price'] = df.price.astype(np.int64)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, let us sort the dataset and browse through some of its rows.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sort_values(by=['year','manufacturer', 'price'], inplace=True)\ndf.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.iloc[np.r_[0:3, -3:0]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the dataset is quite clean now, we can reduce the number of car models in it. We will delete data about models that appear less than 1000 times in the dataset. To exclude most incorrect combinations of manufacturers and models, we will also remove manufacturers that are listed less than 1000 times.\n\nThese steps will dramatically reduce the chance of unrealistic model names and manufacturer-model combinations appearing in our dataset. They will also ensure that the proposed machine-learning models have enough relevant data to understand the interrelations between car attributes or characteristics and their prices.\n\nHere is the code that will update the dataset according to the above requirements:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.groupby(\"model\").filter(lambda x: len(x) >= 1000)\ndf = df.groupby(\"manufacturer\").filter(lambda x: len(x) >= 1000)\ndf.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The standard summary statistics of the updated dataset are as follows:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What are the rounded mean, median, and mode of the target variable, price?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean: ', round(df.price.mean()))\nprint()\nprint('Median: ', round(df.price.median()))\nprint()\nprint('Mode: ', df.price.mode()[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean is greater than the median, and the median is greater than the mode. This indicates that most prices in the price column are on the lower side and a few are much higher. The following histogram proves this:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_histogram(df.price)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Later in the project, we will make sure that we pass on logarithmic versions of these prices to our machine-learning models. For now, though, let us look further into the cleaned data that we&nbsp;have.\n\nFirst, we will look at the number of cars per manufacturer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 4))\nplt.xticks(rotation=90)\nsns.countplot(df.manufacturer);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, let us look at the counts of some of the other categorical variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categ_x = categorical.copy()\ncateg_x.remove('region')\ncateg_x.remove('state')\n\nfig, ax = plt.subplots(3, 3, figsize=(20, 15))\nfor variable, subplot in zip(categ_x, ax.flatten()):\n    sns.countplot(df[variable], ax=subplot)\n    for label in subplot.get_xticklabels():\n        label.set_rotation(90)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graphs above tells us that a majority of cars are in fair condition, are gas powered, have five cylinders, and have automatic transmission.\n\nNext, let us look at how prices are interrelated with various categorical variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 4))\nplt.xticks(rotation=90)\nsns.barplot(x='manufacturer', y='price', data=df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the graph above indicates, Ram Trucks lead on the price front.\n\nAnd the following graphs show that new cars, diesel cars, cars that have liens on them, and cars with four-wheel drives have higher average prices than other types of cars in their respective categories.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(3, 3, figsize=(20, 15))\nfor var, subplot in zip(categ_x, ax.flatten()):\n    sns.barplot(x=var, y='price', data=df, ax=subplot)\n    for label in subplot.get_xticklabels():\n        label.set_rotation(90)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us also look at how prices vary with year of manufacture.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"year = df.year.astype(np.int64)\nprice = df.price\nplt.figure(figsize=(10, 4))\nplt.xticks(rotation=90)\nsns.barplot(year, price);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above graph shows that prices increase fairly consistently with year.\n\nWhat about comparing three sets of variables? How are the fuel and price variables related to condition? The following graphs provide an answer to this question, as well as questions about other triadic combinations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"factor_combos = [('fuel', 'condition'), ('condition', 'size'),\\\n                 ('fuel', 'cylinders'), ('transmission', 'size'),\\\n                 ('size', 'drive'), ('drive', 'size')]\nfig, ax = plt.subplots(3, 2, figsize=(20, 15))\nfor var, subplot in zip(factor_combos, ax.flatten()):\n    sns.barplot(x=var[0], y='price', hue=var[1], data=df, ax=subplot)\n    for label in subplot.get_xticklabels():\n        label.set_rotation(90)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, all the car characteristics in our dataset have some impact on the target variable, price. To determine the interdependence among these variables mathematically, let us calculate the correlation between them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation varies between -1 and 1. A positive correlation value indicates direct proportion, and a negative correlation value indicates inverse proportion.\n\nIf a pair of values are positively correlated, they increase or decrease in unison. Consider the year of manufacture and the price of a car. These values are positively correlated. The more recent or higher the year of manufacture of a vehicle is, the higher is its price.\n\nIf a pair of values are negatively correlated, they travel on the Cartesian plane in opposite directions. If one value increases, the other decreases. Odometer readings and car prices are inversely correlated. The higher the odometer reading of a car is, the lower is its price.\n\nThe strength of a correlation depends on how close the absolute value of the correlation is to 1. For example, a correlation of 0.8 denotes a strong positive correlation. The -0.8 is indicative of a strong negative correlation.\n\nThe correlation table above indicates average positive correlation between price and year and average negative correlation between price and odometer reading.\n\nThe proposed machine-learning models should be able to learn effectively from the car dataset if we allocate adequate data to train and test them. In the next step, we will work in this direction.\n\nBefore we proceed to the next step, let us save the updated dataset as a CSV&nbsp;file.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv('vehicles_viz.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Step 5: Splitting the dataset<a aria-pressed=\"true\" class=\"btn btn-primary btn-sm\" data-toggle=\"popover\" id='step5-desc' href=\"#step5\" role=\"button\" style=\"color: white; margin-left: 20px; target='_self'\">Go to steplist</a></h2>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will begin this step by importing the CSV file that we had saved in the previous&nbsp;step.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('vehicles_viz.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will drop the price column because the logprice column, whose values are uniformly distributed, is more conducive to machine learning.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('price', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will also update the list of categorical columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\ncategorical = list((Counter(df.columns) -\\\n                    Counter(numerical + ['logprice', 'price'])).elements())\ncategorical","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The list of numerical columns is already up to date.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Machine-learning algorithms can interpret numerical values with a high degree of precision. But these algorithms can misinterpret categorical data. Therefore, we must convert the categorical data in our dataset into numerical values.\n\nThe following code can perform the conversion:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the encoder instance of preprocessing.LabelEncoder(),\n# which we had declared earlier in this noebook\n\ndf[categorical] = df[categorical].apply(encoder.fit_transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here are the first few rows of the updated dataset:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we need to rescale and uniformize the values in selected columns. If these values are uniformly distributed, machine-learning algorithms will process them efficiently.\n\nThe logprice column is already uniformly distributed. To uniformize the other numerical columns, we will use the StandardScaler module of scikit-learn. StandardScaler can rescale a set of numerical values, giving the set a mean of 0 and a standard deviation of&nbsp;1.\n\nHere is code that implements our requirements:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_to_scale = numerical + ['model', 'region']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nfor col in columns_to_scale:\n    df[col] = scaler.fit_transform(np.array(df[col]).reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having normalized relevant values, we will now apply the IQR method to remove any additional outliers from the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1 = df.logprice.quantile(0.25)\nQ3 = df.logprice.quantile(0.75)\nIQR = Q3 - Q1\n\ndf = df[(df.logprice >= (Q1 - 1.5 * IQR)) & (df.logprice <= (Q3 + 1.5 * IQR))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us look at the first few rows of the updated dataset:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we will split the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df.iloc[:, :-1]\ny = df.iloc[:,-1:].values.T[0]\n\nX_train, X_test, y_train, y_test =\\\ntrain_test_split(X, y, train_size=0.9, test_size=0.1, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we proceed to the next step, let us write a couple of helper functions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_negatives(y_test, y_pred):\n    '''This function will remove any negative values from predictions'''\n    ind = [index for index in range(len(y_pred)) if(y_pred[index]>0)]\n    y_pred = y_pred[ind]\n    y_test = y_test[ind]\n    return (y_test, y_pred)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error,\\\nr2_score\n\ndef evaluate_perf(y_test, y_pred):\n    '''This function will generate metrics about the\n    performance of a model.'''\n    res = []\n    res.append(mean_squared_log_error(y_test, y_pred))\n    res.append(np.sqrt(res[0]))\n    res.append(r2_score(y_test, y_pred))\n    res.append(round(r2_score(y_test, y_pred)*100, 4))\n    return (res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will also create a dataframe to store any performance metrics that the `evaluate_perf` function returns. This dataframe will contain four&nbsp;rows.\n\nAmong the row indexes of the dataframe, MSLE stands for mean squared logarithmic error and RMSLE is root MSLE. We can use MSLE and RMSLE to determine the variation between actual target values and target values that are predicted by a machine-learning model.\n\nThe third index, R2 score, refers to the r-squared (R2) statistical measure. Like MSLE, R2 also signifies the variance between predicted and actual values. R2 \"is the proportion of the variance in the dependent variable that is predictable from the independent variable(s)<sup id=\"r2\" style=\"font-size: x-small\">[[1]](#r2-wiki)</sup>\"\n\nR2 scores are restricted to the range 0–1. A positive R2 score means that the machine-learning algorithm that we are evaluating is effective. An R2 score of 0 means that the machine-learning model is just returning the average of the actual variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_metrics = pd.DataFrame(index=['MSLE', 'RMSLE',\\\n                                 'R2 score','Accuracy(%)'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Step 6: Building and validating models<a aria-pressed=\"true\" class=\"btn btn-primary btn-sm\" data-toggle=\"popover\" id='step6-desc' href=\"#step6\" role=\"button\" style=\"color: white; margin-left: 20px; target='_self'\">Go to steplist</a></h2>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Our first model will be a linear regressor.\n\n### Building the linear regressor\n\nWe will use the following code to build the model:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\n# Instantiating a linear regressor\nlin_reg = LinearRegression()\n\n# Fitting a linear-regression model\nlin_reg.fit(X_train, y_train)\ny_pred = lin_reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Validating the linear regressor\n\nHere is code that will validate the model:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Validating the model by identifying errors\ny_test_1, y_pred_1 = remove_negatives(y_test, y_pred)\nres_lin_reg = evaluate_perf(y_test_1, y_pred_1)\n\nprint(\"Coefficients: \\n\", lin_reg.coef_)\nprint(f\"MSLE : {res_lin_reg[0]}\")\nprint(f\"Root MSLE : {res_lin_reg[1]}\")\nprint(f\"R2 Score : {res_lin_reg[2]} or {res_lin_reg[3]}%\")\n\ndf_metrics[\"Linear\"] = res_lin_reg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How did the actual and predicted values vary? Let us draw a bar graph to compare some of these values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lin_comp = pd.DataFrame({'Actual': y_test_1, 'Predicted': y_pred_1})\ndf_lin_comp = df_lin_comp.head(25)\n\ndf_lin_comp.plot(kind='bar', figsize=(10,5))\n\nplt.grid(which='major', linestyle='-', linewidth='0.1', color='green')\nplt.title('Linear regressor: Actual vs. predicted')\nplt.ylabel('MSLE')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, let us see how the linear regressor ranked independent variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"coefs = pd.Series(lin_reg.coef_, index = X_train.columns)\nsorted_coefs = coefs.sort_values()\n\nsorted_coefs.plot(kind = \"barh\")\n\nplt.rcParams['figure.figsize'] = (6.0, 6.0)\nplt.xlabel('Score'); \nplt.ylabel('Feature'); \nplt.title('Linear regressor: Feature importance')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will work on a random-forest regressor.\n\n## Building the random-forest regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrf_reg = RandomForestRegressor(n_estimators=180,\n                               random_state=0,\n                               min_samples_leaf=1,\n                               max_features=0.5,\n                               n_jobs=-1,\n                               oob_score=True)\n\nrf_reg.fit(X_train,y_train)\ny_pred = rf_reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Validating the random-forest regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_1, y_pred_1 = remove_negatives(y_test, y_pred)\nres_rf_reg = evaluate_perf(y_test_1, y_pred_1)\n\nprint(f\"MSLE : {res_rf_reg[0]}\")\nprint(f\"Root MSLE : {res_rf_reg[1]}\")\nprint(f\"R2 Score : {res_rf_reg[2]} or {res_rf_reg[3]}%\")\n\ndf_metrics['RandomForest'] = res_rf_reg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How did the actual and predicted values vary? Let us draw a bar graph to compare some of these values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_rf_comp = pd.DataFrame({'Actual': y_test_1, 'Predicted': y_pred_1})\ndf_rf_comp = df_rf_comp.head(25)\n\ndf_rf_comp.plot(kind='bar', figsize=(10,5))\n\nplt.grid(which='major', linestyle='-', linewidth='0.1', color='green')\nplt.title('Random forest: Actual vs. predicted')\nplt.ylabel('MSLE')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us also depict the random-forest ranking of independent variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = rf_reg.feature_importances_\nfeatures = X_train.columns\nx_vals = list(range(len(importances)))\n\nplt.figure(figsize=(6,6))\nplt.bar(x_vals, importances, orientation = ('vertical'))\n\nplt.xticks(x_vals, features, rotation=90)\nplt.ylabel('Score'); \nplt.xlabel('Feature'); \nplt.title('Random forest: Feature importance')\n\nplt.tight_layout()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we will work on an XGBoost regressor.\n\n## Building the XGBoost regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nxg_reg = xgb.XGBRegressor(objective ='reg:squarederror',\n                          learning_rate = 0.4,\n                          max_depth = 24,\n                          alpha = 5,\n                          n_estimators = 200)\n\nxg_reg.fit(X_train,y_train)\ny_pred = xg_reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Validating the XGBoost regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_1, y_pred_1 = remove_negatives(y_test, y_pred)\nres_xg_reg = evaluate_perf(y_test_1,y_pred_1)\n\nprint(f\"MSLE : {res_xg_reg[0]}\")\nprint(f\"Root MSLE : {res_xg_reg[1]}\")\nprint(f\"R2 Score : {res_xg_reg[2]} or {res_xg_reg[3]}%\")\n\ndf_metrics['XGBoost'] = res_xg_reg\n\n# Saving the metrics dataframe as a CSV file because\n# it contains data about each of the three models now\ndf_metrics.to_csv('error_metrics.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To round off this step, let us depict the XGBoost ranking of independent variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.plot_importance(xg_reg)\n\nplt.rcParams['figure.figsize'] = [5, 5]\nplt.title('XGBoost: Feature importance')\n\nplt.tight_layout()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Step 7: Identifying the most appropriate model<a aria-pressed=\"true\" class=\"btn btn-primary btn-sm\" data-toggle=\"popover\" id='step7-desc' href=\"#step7\" role=\"button\" style=\"color: white; margin-left: 20px; target='_self'\">Go to steplist</a></h2>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As the following table and graph indicate, the linear regressor is the least accurate model. Random forest and XGBoost have performed much better than the linear regressor in multiple iterations of the code. And the accuracies of these high performers have been on a par mostly. \n\nBut if I had to choose one regressor for this type of project, it would be XGBoost. If an XGBoost model is tuned properly, it can reduce bias and variance and give accurate results. What adds to its appeal is that it is optimized for speed and real-world performance.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_metrics = pd.read_csv('error_metrics.csv', index_col=0)\ndf_metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies = df_metrics.loc['Accuracy(%)']\n\nx = list(range(len(accuracies)))\ny = list(range(0, 101, 10))\n\nprops = dict(boxstyle='round', facecolor='white', alpha=0.8)\nplt.figure(figsize=(20, 6))\n\nplt.plot(accuracies)\n\nplt.yticks(y)\nplt.xticks(fontsize=20)\nplt.xlabel(\"Model\", fontsize=30)\nplt.ylabel(\"Accuracy(%)\", fontsize=30)\nplt.title(\"Accuracies of models\")\n\nfor a, b in zip(x, y):\n    b = accuracies[a]\n    val=\"(\" + str(round(accuracies[a], 2)) + \" %)\"\n    plt.text(a, b + 4.5, val, horizontalalignment='center',\\\n             verticalalignment='center', color='green', bbox=props)\n    plt.text(a, b + 3.5, '.', horizontalalignment='center',\\\n             verticalalignment='center', color='red', fontsize=50)\n    \nplt.tight_layout()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## References\n\n- Panwar Abhash Anil, \"Used Car Price Prediction using Machine Learning,\" Medium (blog), Towards Data Science, August 3, 2020, https://towardsdatascience.com/used-car-price-prediction-using-machine-learning-e3be02d977b2.\n\n- Soner Yıldırım, \"Predicting Used Car Prices with Machine Learning,\" Medium (blog), Towards Data Science, January 28, 2020, https://towardsdatascience.com/predicting-used-car-prices-with-machine-learning-fea53811b1ab.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Footnote\n\n<a name=\"r2-wiki\">1</a>: \"Coefficient of determination,\" Wikimedia Foundation, last modified September 4, 2020, 04:49, https://en.wikipedia.org/wiki/Coefficient_of_determination. [↩](#r2)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}